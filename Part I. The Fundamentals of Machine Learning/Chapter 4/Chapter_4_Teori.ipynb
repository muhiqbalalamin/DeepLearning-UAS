{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+yCTOU9ro6fsn5aF1QblK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhiqbalalamin/DeepLearning-UAS/blob/main/Chapter_4_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Models**"
      ],
      "metadata": {
        "id": "S9d-FRMLNE2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pendahuluan"
      ],
      "metadata": {
        "id": "gcXy2Y1-NGw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bab ini membahas proses di balik layar saat melatih model Machine Learning, dengan fokus pada topik-topik berikut:\n",
        "\n",
        "* Regresi Linier\n",
        "\n",
        "* Gradient Descent\n",
        "\n",
        "* Regularisasi\n",
        "\n",
        "* Regresi Logistik\n",
        "\n",
        "* Regresi Softmax"
      ],
      "metadata": {
        "id": "aiyqbgznNQ3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresi Linier"
      ],
      "metadata": {
        "id": "Urx0I1kdNVca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model regresi linier memprediksi nilai target sebagai kombinasi linier dari fitur-fitur masukan.\n",
        "\n",
        "### Rumus Regresi Linier\n",
        "\\[\n",
        "$\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + \\theta_nx_n$\n",
        "\\]\n",
        "\n",
        "atau dalam bentuk vektor:\n",
        "\n",
        "\\[\n",
        "$\\hat{y} = \\mathbf{\\theta}^T \\cdot \\mathbf{x}$\n",
        "\\]\n",
        "\n",
        "\n",
        "### MSE (Mean Squared Error)\n",
        "Fungsi kerugian yang digunakan untuk regresi linier:\n",
        "\n",
        "\\[\n",
        "$\\text{MSE}(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left( \\theta^T x^{(i)} - y^{(i)} \\right)^2$\n",
        "\\]"
      ],
      "metadata": {
        "id": "7T3CxcSANYU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal Equation"
      ],
      "metadata": {
        "id": "vf4vcJSLN2V5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solusi eksak untuk parameter regresi linier diperoleh dengan rumus:\n",
        "\n",
        "\\[\n",
        "$\\theta = (X^T X)^{-1} X^T y$\n",
        "\\]\n",
        "\n",
        "Namun, metode ini menjadi tidak efisien ketika jumlah fitur sangat besar karena kompleksitas komputasi yang tinggi."
      ],
      "metadata": {
        "id": "xkjp4erZN-GR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent"
      ],
      "metadata": {
        "id": "QjoFSgbtOK8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prinsip Dasar\n",
        "Gradient Descent adalah metode iteratif untuk meminimalkan fungsi kerugian.\n",
        "\n",
        "Rumus Pembaruan\n",
        "\n",
        "\\[\n",
        "$\\theta := \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta)$\n",
        "\\]\n",
        "\n",
        "- \\($ \\eta $\\): learning rate\n",
        "- \\($ \\nabla_\\theta J(\\theta) $\\): turunan dari fungsi kerugian terhadap parameter\n",
        "\n",
        "### Jenis-Jenis Gradient Descent:\n",
        "\n",
        "1. Batch Gradient Descent ‚Äì menggunakan seluruh dataset\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD) ‚Äì menggunakan satu sampel per iterasi\n",
        "\n",
        "3. Mini-batch Gradient Descent ‚Äì menggunakan subset kecil dari data"
      ],
      "metadata": {
        "id": "jAyObKWNOMnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresi Polinomial"
      ],
      "metadata": {
        "id": "wSvY_E9LO7zT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model ini memproyeksikan data ke fitur-fitur polinomial agar mampu mempelajari hubungan nonlinier.\n",
        "\n",
        "Tantangan:\n",
        "* Overfitting jika derajat polinomial terlalu tinggi\n",
        "\n",
        "* Underfitting jika derajat terlalu rendah"
      ],
      "metadata": {
        "id": "r9uLBxarO816"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Curves"
      ],
      "metadata": {
        "id": "kXfb50ItPDl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kurva pembelajaran menggambarkan error terhadap ukuran data pelatihan untuk mendeteksi overfitting atau underfitting.\n",
        "\n",
        "* Training error turun\n",
        "\n",
        "* Validation error naik ‚Üí indikasi overfitting"
      ],
      "metadata": {
        "id": "8PWdyTWNPFr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularisasi\n",
        "Regularisasi bertujuan mencegah overfitting dengan menambahkan penalti terhadap nilai parameter model.\n",
        "\n",
        "Ridge Regression (L2)\n",
        "\n",
        "\\[\n",
        "$J(\\theta) = \\text{MSE}(\\theta) + \\alpha \\sum_{j=1}^{n} \\theta_j^2$\n",
        "\\]\n",
        "\n",
        "Lasso Regression (L1)\n",
        "\n",
        "\\[\n",
        "$J(\\theta) = \\text{MSE}(\\theta) + \\alpha \\sum_{j=1}^{n} |\\theta_j|$\n",
        "\\]\n",
        "\n",
        "Elastic Net\n",
        "Gabungan L1 dan L2:\n",
        "\n",
        "\\[\n",
        "$J(\\theta) = \\text{MSE}(\\theta) + r \\cdot \\sum |\\theta_j| + (1 - r) \\cdot \\sum \\theta_j^2$\n",
        "\\]"
      ],
      "metadata": {
        "id": "30tTuaPQPJcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping\n",
        "Teknik ini menghentikan proses pelatihan saat error pada validation set mulai meningkat, untuk menghindari overfitting."
      ],
      "metadata": {
        "id": "lIR5hfHpPteh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresi Logistik\n",
        "Model ini digunakan untuk klasifikasi biner.\n",
        "\n",
        "### Hipotesis (Fungsi Sigmoid)\n",
        "\n",
        "\\[\n",
        "$\\hat{p} = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\quad \\text{dengan } z = \\theta^T x$\n",
        "\\]\n",
        "\n",
        "### Fungsi Kerugian (Log Loss)\n",
        "\n",
        "\\[\n",
        "$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{p}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{p}^{(i)}) \\right]$\n",
        "\\]\n",
        "\n",
        "### Turunan parsial terhadap parameter:\n",
        "\n",
        "\\[\n",
        "$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{p}^{(i)} - y^{(i)} \\right) x_j^{(i)}$\n",
        "\\]"
      ],
      "metadata": {
        "id": "0qpfod7wPvWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresi Softmax (Multikelas)\n",
        "Regresi Softmax merupakan perluasan regresi logistik untuk klasifikasi multi-kelas.\n",
        "\n",
        "### Skor untuk kelas ke- ùëò:\n",
        "\n",
        "\\[\n",
        "$s_k(x) = x^T \\theta^{(k)}$\n",
        "\\]\n",
        "\n",
        "\n",
        "### Fungsi Softmax:\n",
        "\n",
        "\\[\n",
        "$\\hat{p}_k = \\frac{e^{s_k(x)}}{\\sum_{j=1}^{K} e^{s_j(x)}}$\n",
        "\\]\n",
        "\n",
        "### Fungsi Kerugian (Cross-Entropy):\n",
        "\n",
        "\\[\n",
        "$J(\\Theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{p}_k^{(i)})$\n",
        "\\]\n",
        "\n",
        "### Gradien:\n",
        "\n",
        "\\[\n",
        "$\\nabla_{\\theta^{(k)}} J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{p}_k^{(i)} - y_k^{(i)} \\right) x^{(i)}$\n",
        "\\]"
      ],
      "metadata": {
        "id": "1eIdXnd-QP1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularisasi pada Regresi Logistik/Softmax\n",
        "Secara default, LogisticRegression di Scikit-Learn menggunakan regularisasi L2.\n",
        "\n",
        "* Hyperparameter: C adalah kebalikan dari kekuatan regularisasi\n",
        "\n",
        "    *  Nilai C besar ‚Üí regularisasi lemah\n",
        "\n",
        "    *  Nilai C kecil ‚Üí regularisasi kuat"
      ],
      "metadata": {
        "id": "D2Y8RaXVQ5br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kesimpulan**\n",
        "Bab ini membahas berbagai teknik dasar yang digunakan dalam pelatihan model Machine Learning, dimulai dari regresi linier hingga regresi logistik dan softmax. Regresi linier dapat diselesaikan secara analitik menggunakan Normal Equation atau secara iteratif dengan Gradient Descent, yang memiliki tiga varian: batch, stochastic, dan mini-batch. Untuk menangani permasalahan overfitting, diterapkan teknik regularisasi seperti Ridge (L2), Lasso (L1), dan Elastic Net, serta strategi early stopping. Dalam klasifikasi, regresi logistik digunakan untuk kasus biner, sedangkan regresi softmax digunakan untuk kasus multikelas. Kedua pendekatan ini menggunakan fungsi kerugian yang sesuai seperti log loss dan cross-entropy, serta melibatkan proses optimasi melalui turunan gradien. Pemahaman menyeluruh terhadap rumus matematis, fungsi kerugian, serta metode optimasi sangat penting untuk membangun model yang efektif, efisien, dan mampu melakukan generalisasi dengan baik terhadap data baru."
      ],
      "metadata": {
        "id": "9wOfS9f5RCbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Referensi**\n",
        "G√©ron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O'Reilly Media."
      ],
      "metadata": {
        "id": "E2YqaRzIRMeK"
      }
    }
  ]
}
