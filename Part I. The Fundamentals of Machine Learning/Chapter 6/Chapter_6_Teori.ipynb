{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNKprRCcokEq3yhDIm6elH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhiqbalalamin/DeepLearning-UAS/blob/main/Chapter_6_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Trees**"
      ],
      "metadata": {
        "id": "eYqeD-7cbgJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apa Itu Decision Tree?\n",
        "Decision Tree adalah algoritma pembelajaran yang dapat digunakan untuk tugas klasifikasi maupun regresi. Algoritma ini sangat intuitif, mudah divisualisasikan, dan tidak memerlukan proses feature scaling.\n",
        "\n",
        "## Pelatihan dan Visualisasi Decision Tree\n",
        "Pohon keputusan dilatih dengan membagi dataset berdasarkan aturan keputusan biner yang didasarkan pada fitur input. Proses ini berlangsung secara rekursif hingga mencapai kondisi penghentian tertentu, seperti kedalaman maksimum atau node yang sudah sepenuhnya homogen (pure).\n",
        "\n",
        "## Mekanisme Prediksi\n",
        "Prediksi dilakukan dengan menelusuri pohon dari akar hingga ke daun:\n",
        "\n",
        "* Setiap node mengajukan pertanyaan (misalnya, “Apakah petal length ≤ 2.45?”).\n",
        "\n",
        "* Berdasarkan jawaban (ya/tidak), algoritma bergerak ke anak node yang sesuai.\n",
        "\n",
        "* Setelah mencapai leaf node, hasil prediksi ditentukan berdasarkan kelas mayoritas (untuk klasifikasi) atau nilai rata-rata (untuk regresi).\n",
        "\n",
        "## Estimasi Probabilitas\n",
        "Untuk tugas klasifikasi, Decision Tree juga dapat menghasilkan estimasi probabilitas:\n",
        "\n",
        "\\[\n",
        "$P(k) = \\frac{\\text{Jumlah instance kelas } k}{\\text{Total instance di node}}$\n",
        "\\]\n",
        "\n",
        "## Ukuran Ketidakmurnian (Impurity Measures)\n",
        "### Gini Impurity\n",
        "\n",
        "\\[\n",
        "$G_i = 1 - \\sum_{k=1}^n p_{i,k}^2$\n",
        "\\]\n",
        "\n",
        "* Gini = 0 berarti node sudah pure.\n",
        "\n",
        "* Merupakan ukuran ketidakmurnian default di Scikit-Learn.\n",
        "\n",
        "### Entropy\n",
        "\n",
        "\\[\n",
        "$H_i = - \\sum_{k=1}^{n} p_{i,k} \\log_2(p_{i,k})$\n",
        "\\]\n",
        "\n",
        "* Lebih mahal secara komputasi dibandingkan Gini.\n",
        "\n",
        "* Cenderung menghasilkan pohon yang lebih seimbang."
      ],
      "metadata": {
        "id": "8UCidK6dbhQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritma CART (Classification And Regression Tree)\n",
        "Scikit-Learn menggunakan algoritma CART untuk membangun pohon keputusan.\n",
        "\n",
        "Untuk Klasifikasi\n",
        "\n",
        "\\[\n",
        "$J(k, t_k) = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m} G_{right}$\n",
        "\\]\n",
        "\n",
        "- Memilih fitur \\($k$\\) dan threshold \\($t_k$\\) yang meminimalkan impurity gabungan.\n",
        "\n",
        "## Kompleksitas Waktu\n",
        "\n",
        "- **Prediksi**: \\($O(\\log_2 m)$\\)\n",
        "- **Training**: \\($O(n \\cdot m \\cdot \\log_2 m)$\\)\n",
        "  - \\($n$\\): jumlah fitur\n",
        "  - \\($m$\\): jumlah instance\n",
        "\n",
        "## Regularisasi\n",
        "Untuk mencegah overfitting, beberapa parameter pembatas dapat diterapkan:\n",
        "\n",
        "* max_depth\n",
        "\n",
        "* min_samples_split\n",
        "\n",
        "* min_samples_leaf\n",
        "\n",
        "* max_leaf_nodes\n",
        "\n",
        "* max_features\n",
        "\n",
        "Model tanpa pembatasan cenderung terlalu kompleks dan mudah overfit.\n",
        "\n",
        "## Pruning\n",
        "Sebagai alternatif regularisasi saat pelatihan, dapat dilakukan post-pruning:\n",
        "\n",
        "* Setelah pohon selesai dibentuk, node-node yang kontribusinya tidak signifikan dihapus.\n",
        "\n",
        "* Dapat menggunakan uji statistik seperti chi-squared test."
      ],
      "metadata": {
        "id": "YyNLsCPhcGRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree untuk Regresi\n",
        "* Konsep serupa dengan klasifikasi, namun leaf node menyimpan nilai rata-rata target.\n",
        "\n",
        "* CART untuk regresi meminimalkan MSE (Mean Squared Error):\n",
        "\n",
        "\\[\n",
        "$J(k, t_k) = \\frac{m_{left}}{m} MSE_{left} + \\frac{m_{right}}{m} MSE_{right}$\n",
        "\\]\n",
        "\n",
        "Dengan:\n",
        "\n",
        "\\[\n",
        "$MSE = \\frac{1}{m_{node}} \\sum_{i \\in node} (y_i - \\bar{y})^2$\n",
        "\\]"
      ],
      "metadata": {
        "id": "JSA9jldxckAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kelemahan Decision Tree\n",
        "1. Overfitting: Rentan terlalu menyesuaikan data pelatihan jika tidak diregularisasi.\n",
        "\n",
        "2. Tidak stabil: Perubahan kecil pada data dapat menghasilkan struktur pohon yang sangat berbeda.\n",
        "\n",
        "3. Sensitif terhadap rotasi fitur: Karena pemisahan hanya dilakukan secara tegak lurus terhadap sumbu.\n",
        "\n",
        "4. Model ortogonal: Tidak efisien untuk batas keputusan yang miring.\n",
        "\n",
        "## White Box Model\n",
        "Decision Tree disebut sebagai white box model karena keputusan yang dihasilkan dapat ditelusuri dan dijelaskan secara transparan. Hal ini berbeda dengan black box model seperti Neural Networks atau Random Forests yang lebih sulit diinterpretasikan."
      ],
      "metadata": {
        "id": "43mPgLjbcyYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kesimpulan**\n",
        "Decision Tree merupakan algoritma pembelajaran yang fleksibel dan intuitif, mampu menangani berbagai tugas seperti klasifikasi, regresi, hingga multioutput. Dengan struktur pohon yang mudah divisualisasikan dan ditelusuri, model ini termasuk dalam kategori white box sehingga sangat cocok digunakan ketika interpretabilitas menjadi prioritas. Scikit-Learn mengimplementasikan Decision Tree menggunakan algoritma CART yang membagi data secara biner berdasarkan fitur dan threshold yang meminimalkan impurity. Meskipun Decision Tree tidak memerlukan normalisasi fitur dan dapat bekerja dengan data dalam bentuk mentah, model ini memiliki kelemahan seperti rentan terhadap overfitting, sensitif terhadap perubahan kecil pada data, dan kurang efisien untuk batas keputusan yang tidak tegak lurus. Oleh karena itu, penerapan teknik regularisasi seperti pengaturan kedalaman maksimum atau jumlah sampel minimum sangat penting untuk menjaga generalisasi model. Dengan kelebihan dan kekurangannya, Decision Tree tetap menjadi pilihan yang kuat dan mudah digunakan dalam banyak kasus pembelajaran mesin."
      ],
      "metadata": {
        "id": "MiQoRgB6c8CE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Referensi**\n",
        "Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O'Reilly Media."
      ],
      "metadata": {
        "id": "5SQd1Tsnc52k"
      }
    }
  ]
}
