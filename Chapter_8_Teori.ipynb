{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuBQebKKQgGauNMA1eDfzO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhiqbalalamin/DeepLearning-UAS/blob/main/Chapter_8_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dimensionality Reduction**"
      ],
      "metadata": {
        "id": "kWk_CcJokrGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mengapa Perlu Mengurangi Dimensi?\n",
        "Banyak permasalahan Machine Learning melibatkan ratusan hingga jutaan fitur. Hal ini dapat menyebabkan:\n",
        "\n",
        "* Proses pelatihan menjadi lambat\n",
        "\n",
        "* Risiko overfitting meningkat\n",
        "\n",
        "* Akurasi generalisasi menurun\n",
        "\n",
        "Tujuan utama dari reduksi dimensi meliputi:\n",
        "\n",
        "* Mempercepat proses pelatihan\n",
        "\n",
        "* Mengurangi noise\n",
        "\n",
        "* Memungkinkan visualisasi dalam 2D/3D\n",
        "\n",
        "* Kompresi data\n",
        "\n",
        "## The Curse of Dimensionality\n",
        "Fenomena yang tidak intuitif dalam ruang berdimensi tinggi, antara lain:\n",
        "\n",
        "* Titik-titik data menjadi jarang tersebar\n",
        "\n",
        "* Rata-rata jarak antar titik meningkat drastis\n",
        "\n",
        "* Semakin tinggi dimensi, semakin tidak andal hasil prediksi\n",
        "\n",
        "Contoh:\n",
        "\n",
        "* Di ruang berdimensi satu juta, jarak rata-rata antar titik dapat mencapai ~408,25\n",
        "\n",
        "* Dibutuhkan jumlah data yang eksponensial untuk mempertahankan kepadatan data yang memadai\n",
        "\n",
        "## Dua Pendekatan Utama\n",
        "### Projection\n",
        "Sering kali, data berada dalam subruang berdimensi lebih rendah. Oleh karena itu, kita dapat:\n",
        "\n",
        "* Mengidentifikasi subruang tersebut\n",
        "\n",
        "* Memproyeksikan data secara ortogonal ke dalamnya\n",
        "\n",
        "Namun, pendekatan ini kurang efektif untuk manifold non-linier (misalnya: Swiss Roll).\n",
        "\n",
        "### Manifold Learning\n",
        "Asumsi manifold menyatakan bahwa data dunia nyata cenderung terletak pada manifold berdimensi rendah di dalam ruang berdimensi tinggi.\n",
        "\n",
        "Contoh:\n",
        "\n",
        "* MNIST: angka tulisan tangan membentuk jalur kontinu dalam ruang fitur\n",
        "\n",
        "* Swiss Roll: struktur lokalnya dua dimensi, tetapi terlipat dalam ruang tiga dimensi"
      ],
      "metadata": {
        "id": "nQXasJtskucF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA – Principal Component Analysis\n",
        "### Tujuan\n",
        "Menemukan subruang yang mempertahankan variansi data sebanyak mungkin.\n",
        "\n",
        "### Langkah-langkah PCA\n",
        "### 1. Singular Value Decomposition (SVD)\n",
        "\n",
        "\\[\n",
        "$X = U \\Sigma V^T$\n",
        "\\]\n",
        "\n",
        "- \\($ V $\\): matriks komponen utama (PC)\n",
        "- \\($ X $\\): data yang sudah di-center-kan\n",
        "\n",
        "### 2. Proyeksi ke Subruang\n",
        "\n",
        "\\[\n",
        "$X_{proj} = X \\cdot W_d$\n",
        "\\]\n",
        "\n",
        "- \\($ W_d $\\): d kolom pertama dari \\($ V $\\)\n",
        "\n",
        "## Explained Variance Ratio (EVR)\n",
        "\n",
        "\\[\n",
        "$\\text{EVR}_k = \\frac{\\text{Var}(PC_k)}{\\text{Total Variance}}$\n",
        "\\]\n",
        "\n",
        "Jumlah dimensi dipilih berdasarkan seberapa besar rasio variansi yang ingin dipertahankan (misalnya 95%)."
      ],
      "metadata": {
        "id": "syWapuwzlMe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA untuk Kompresi\n",
        "* Mengurangi dimensi memungkinkan penyimpanan data yang lebih efisien\n",
        "\n",
        "* Contoh: MNIST dikompresi dari 784 menjadi 154 fitur, mempertahankan 95% variansi\n",
        "\n",
        "* Data dapat direkonstruksi kembali dengan:\n",
        "\n",
        "\\[\n",
        "$X_{recovered} = X_{proj} \\cdot W_d^T$\n",
        "\\]\n",
        "\n",
        "Rekonstruksi ini disebut sebagai reconstruction pre-image.\n",
        "\n",
        "## Incremental PCA\n",
        "Digunakan ketika dataset terlalu besar untuk dimuat seluruhnya ke dalam memori:\n",
        "\n",
        "* Diproses secara batch\n",
        "\n",
        "* Cocok untuk skenario online learning\n",
        "\n",
        "## Kernel PCA\n",
        "Melakukan PCA dalam ruang berdimensi tinggi menggunakan trik kernel.\n",
        "\n",
        "### Kernel yang Umum Digunakan:\n",
        "* Linear\n",
        "\n",
        "* RBF (Gaussian)\n",
        "\n",
        "* Sigmoid\n",
        "\n",
        "Pemilihan kernel terbaik tidak dapat ditentukan secara pasti — disarankan menggunakan Grid Search berdasarkan performa model supervised (misalnya klasifikasi)."
      ],
      "metadata": {
        "id": "oqeUVc4qll0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLE – Locally Linear Embedding\n",
        "### Langkah 1: Rekonstruksi Lokal\n",
        "\\[\n",
        "$W = \\arg \\min_W \\sum_{i=1}^m \\left\\| x_i - \\sum_j w_{ij} x_j \\right\\|^2$\n",
        "\\]\n",
        "\n",
        "Syarat:\n",
        "- \\($ w_{ij} = 0 $\\) jika \\($ x_j $\\) bukan tetangga terdekat dari \\($ x_i $\\)\n",
        "- \\($ \\sum_j w_{ij} = 1 $\\)\n",
        "\n",
        "### Langkah 2: Mapping ke Ruang Rendah\n",
        "\n",
        "\\[\n",
        "$Z = \\arg \\min_Z \\sum_{i=1}^m \\left\\| z_i - \\sum_j w_{ij} z_j \\right\\|^2$\n",
        "\\]\n",
        "\n",
        "* Hubungan lokal antar titik tetap dipertahankan\n",
        "\n",
        "* Cocok untuk data dengan struktur manifold yang terlipat"
      ],
      "metadata": {
        "id": "1iblbi_Dl5mX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teknik Lain dalam Reduksi Dimensi\n",
        "### Random Projections\n",
        "* Proyeksi acak ke dimensi yang lebih rendah\n",
        "\n",
        "* Jarak antar data tetap dipertahankan berdasarkan Johnson–Lindenstrauss Lemma\n",
        "\n",
        "## Multidimensional Scaling (MDS)\n",
        "* Mempertahankan jarak antar instance\n",
        "\n",
        "## Isomap\n",
        "* Memperluas MDS dengan mempertimbangkan jarak geodesik\n",
        "\n",
        "## t-SNE\n",
        "* Mempertahankan struktur lokal (misalnya: klaster)\n",
        "\n",
        "* Sangat efektif untuk visualisasi data berdimensi tinggi\n",
        "\n",
        "## LDA – Linear Discriminant Analysis\n",
        "* Digunakan dalam klasifikasi\n",
        "\n",
        "* Memproyeksikan data ke dimensi yang memisahkan kelas dengan optimal"
      ],
      "metadata": {
        "id": "qfYMHwTXmHkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kesimpulan**\n",
        "Reduksi dimensi membantu mempercepat pelatihan model dan memungkinkan visualisasi data. PCA sangat efektif untuk kompresi dan reduksi linier, sementara teknik seperti Kernel PCA dan LLE lebih cocok untuk menangani struktur data non-linier. Pemilihan metode reduksi dimensi harus disesuaikan dengan jenis dataset dan tujuan analisis, baik untuk kompresi, visualisasi, maupun preprocessing sebelum pelatihan model.\n",
        "\n",
        "# **Referensi**\n",
        "Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O'Reilly Media."
      ],
      "metadata": {
        "id": "NljQLwspmg73"
      }
    }
  ]
}