{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhi+wDoyab4sJkHS5s5jzo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhiqbalalamin/DeepLearning-UAS/blob/main/Chapter_11_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Deep Neural Networks**"
      ],
      "metadata": {
        "id": "4bHKHkgNyJ9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tujuan Bab\n",
        "Bab ini bertujuan untuk memahami berbagai teknik yang dapat digunakan untuk:\n",
        "\n",
        "* Mempercepat proses pelatihan jaringan saraf dalam (Deep Neural Networks)\n",
        "\n",
        "* Meningkatkan konvergensi model\n",
        "\n",
        "* Mengatasi permasalahan seperti overfitting dan vanishing gradient\n",
        "\n",
        "* Memilih arsitektur serta hyperparameter secara lebih efektif\n",
        "\n",
        "## Tantangan dalam Melatih Deep Neural Networks\n",
        "Beberapa tantangan utama dalam pelatihan jaringan saraf dalam meliputi:\n",
        "\n",
        "1. Vanishing dan Exploding Gradients\n",
        "\n",
        "2. Waktu pelatihan yang lama\n",
        "\n",
        "3. Kesulitan dalam memilih arsitektur yang tepat\n",
        "\n",
        "4. Overfitting\n",
        "\n",
        "## Vanishing dan Exploding Gradients\n",
        "### Masalah:\n",
        "Ketika nilai gradien terlalu kecil (vanishing) atau terlalu besar (exploding), proses pelatihan menjadi tidak stabil dan sulit untuk konvergen.\n",
        "\n",
        "Masalah ini umum terjadi pada jaringan:\n",
        "\n",
        "* Yang terlalu dalam (terdiri dari banyak lapisan)\n",
        "\n",
        "* Menggunakan fungsi aktivasi seperti sigmoid atau tanh\n",
        "\n",
        "### Solusi:\n",
        "1. Menggunakan fungsi aktivasi ReLU\n",
        "\n",
        "2. Inisialisasi bobot yang sesuai\n",
        "\n",
        "3. Menerapkan Batch Normalization\n",
        "\n",
        "4. Menggunakan arsitektur shortcut seperti ResNet\n"
      ],
      "metadata": {
        "id": "gecM1SHjyTM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inisialisasi Bobot\n",
        "### Inisialisasi Xavier (Glorot)\n",
        "\n",
        "\\[\n",
        "$\\text{Var}(w) = \\frac{1}{n_{\\text{inputs}}}$\n",
        "\\]\n",
        "\n",
        "Direkomendasikan untuk fungsi aktivasi sigmoid atau tanh.\n",
        "\n",
        "### Inisialisasi He\n",
        "\n",
        "\\[\n",
        "$\\text{Var}(w) = \\frac{2}{n_{\\text{inputs}}}$\n",
        "\\]\n",
        "\n",
        "Direkomendasikan untuk fungsi aktivasi ReLU dan turunannya.\n",
        "\n",
        "##  Batch Normalization\n",
        "\n",
        "Teknik normalisasi pada output dari layer (aktivasi) agar:\n",
        "\n",
        "* Memiliki rata-rata mendekati 0 dan standar deviasi mendekati 1\n",
        "\n",
        "* Mempercepat proses pelatihan\n",
        "\n",
        "* Mengurangi ketergantungan terhadap inisialisasi bobot\n",
        "\n",
        "### Rumus:\n",
        "\n",
        "\\[\n",
        "$\\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$\n",
        "\\]\n",
        "\\[\n",
        "$y^{(i)} = \\gamma \\hat{x}^{(i)} + \\beta$\n",
        "\\]\n",
        "\n",
        "Keterangan:\n",
        "- \\($ \\mu_B $\\), \\($ \\sigma_B $\\): mean dan std mini-batch\n",
        "- \\($ \\gamma $\\), \\($ \\beta $\\): parameter learnable"
      ],
      "metadata": {
        "id": "vaVF9CxSymKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Optimizers\n",
        "\n",
        "### SGD (Stochastic Gradient Descent)\n",
        "\n",
        "\\[\n",
        "$\\theta := \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta)$\n",
        "\\]\n",
        "\n",
        "### Momentum\n",
        "\n",
        "\\[\n",
        "$v := \\beta v - \\eta \\nabla_\\theta J(\\theta)\n",
        "\\quad\\text{dan}\\quad \\theta := \\theta + v$\n",
        "\\]\n",
        "\n",
        "### Nesterov Accelerated Gradient\n",
        "\n",
        "\\[\n",
        "$\\theta_{\\text{lookahead}} = \\theta + \\beta v$\n",
        "\\]\n",
        "\n",
        "Gradien dihitung dari posisi lookahead untuk antisipasi arah perubahan parameter.\n",
        "\n",
        "### AdaGrad\n",
        "\n",
        "Mengatur learning rate berdasarkan riwayat gradien:\n",
        "\n",
        "\\[\n",
        "$\\theta_j := \\theta_j - \\frac{\\eta}{\\sqrt{G_{jj}} + \\epsilon} \\cdot g_j$\n",
        "\\]\n",
        "\n",
        "### RMSProp\n",
        "\n",
        "Serupa dengan AdaGrad, tetapi menggunakan rata-rata eksponensial dari kuadrat gradien:\n",
        "\n",
        "\\[\n",
        "$E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta) g_t^2$\n",
        "\\]\n",
        "\n",
        "### Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Menggabungkan pendekatan Momentum dan RMSProp:\n",
        "\n",
        "\\[\n",
        "$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "$\\theta := \\theta - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n",
        "\\]\n"
      ],
      "metadata": {
        "id": "oXxc0SQMy_hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping\n",
        "Menghentikan pelatihan secara otomatis ketika nilai loss validasi mulai meningkat:\n",
        "\n",
        "* Mencegah overfitting\n",
        "\n",
        "* Menghemat waktu pelatihan\n",
        "\n",
        "## Penyesuaian Hyperparameter\n",
        "Parameter yang Perlu Disesuaikan:\n",
        "* Learning rate\n",
        "\n",
        "* Ukuran batch\n",
        "\n",
        "* Jumlah hidden layer dan neuron\n",
        "\n",
        "* Fungsi aktivasi\n",
        "\n",
        "* Optimizer\n",
        "\n",
        "Metode Penyesuaian:\n",
        "* Grid Search\n",
        "\n",
        "* Random Search\n",
        "\n",
        "* Bayesian Optimization\n",
        "\n",
        "## Model Checkpointing\n",
        "Teknik untuk menyimpan model terbaik berdasarkan performa validasi:\n",
        "\n",
        "* Berguna untuk melanjutkan pelatihan\n",
        "\n",
        "* Sangat penting untuk tahap produksi\n",
        "\n",
        "## Pretraining\n",
        "Pretraining Tanpa Label (Unsupervised):\n",
        "* Menggunakan Autoencoder atau Restricted Boltzmann Machine sebagai inisialisasi jaringan\n",
        "\n",
        "Transfer Learning:\n",
        "* Memanfaatkan model terlatih sebelumnya (contoh: model dari ImageNet)\n",
        "\n",
        "* Dilanjutkan dengan fine-tuning pada tugas baru"
      ],
      "metadata": {
        "id": "91GBvBgFzVGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularisasi dan Penambahan Noise\n",
        "### Dropout\n",
        "Selama pelatihan, beberapa neuron dinonaktifkan secara acak dengan probabilitas \\( p \\).\n",
        "\n",
        "\\[\n",
        "$\\tilde{h}_i = h_i \\cdot r_i \\quad \\text{dengan } r_i \\sim \\text{Bernoulli}(p)$\n",
        "\\]\n",
        "\n",
        "Saat inferensi, nilai aktivasi disesuaikan dengan \\($ p $\\).\n"
      ],
      "metadata": {
        "id": "w1nIZ6-Czom4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kesimpulan**\n",
        "* Pelatihan jaringan saraf dalam menghadapi tantangan seperti vanishing gradients, exploding gradients, dan overfitting.\n",
        "\n",
        "* Berbagai teknik seperti ReLU, inisialisasi He, Batch Normalization, dan optimizer modern dapat memperbaiki konvergensi dan stabilitas pelatihan.\n",
        "\n",
        "* Regularisasi seperti dropout dan early stopping meningkatkan kemampuan generalisasi.\n",
        "\n",
        "* Teknik praktis seperti transfer learning dan model checkpointing sangat berguna dalam pengembangan model di dunia nyata.\n",
        "\n",
        "# **Referensi**\n",
        "GÃ©ron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O'Reilly Media."
      ],
      "metadata": {
        "id": "uzTe1Xwvz2F4"
      }
    }
  ]
}