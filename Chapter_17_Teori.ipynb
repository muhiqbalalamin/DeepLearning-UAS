{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9u56E9jV1MntTlRDL7/oS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhiqbalalamin/DeepLearning-UAS/blob/main/Chapter_17_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformers and Pretraining**"
      ],
      "metadata": {
        "id": "mZay_t0mY0hR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tujuan Bab\n",
        "Bab ini bertujuan untuk memahami arsitektur Transformer, yang menjadi fondasi dari berbagai model NLP modern seperti:\n",
        "\n",
        "* BERT\n",
        "\n",
        "* GPT\n",
        "\n",
        "* T5\n",
        "\n",
        "Topik yang dibahas mencakup konsep pretraining dan fine-tuning pada model berskala besar.\n",
        "\n",
        "## Keterbatasan RNN dan CNN dalam NLP\n",
        "RNN:\n",
        "\n",
        "* Sulit untuk diparalelkan\n",
        "\n",
        "* Rentan terhadap vanishing gradient\n",
        "\n",
        "* Tidak efisien dalam memproses sekuens panjang\n",
        "\n",
        "CNN:\n",
        "\n",
        "* Kurang efektif dalam menangkap dependensi jangka panjang\n",
        "\n",
        "Transformer hadir sebagai solusi dengan menerapkan Self-Attention dan pemrosesan paralel.\n",
        "\n",
        "## Gambaran Umum Transformer\n",
        "Diperkenalkan oleh Vaswani et al. (2017) dalam paper \"Attention Is All You Need\".\n",
        "\n",
        "### Komponen Utama:\n",
        "1. Encoder–Decoder:\n",
        "\n",
        "    * Encoder: memproses urutan input\n",
        "\n",
        "    * Decoder: menghasilkan urutan output\n",
        "\n",
        "2. Self-Attention:\n",
        "\n",
        "    * Setiap posisi dalam input dapat memerhatikan posisi lain dalam urutan\n",
        "\n",
        "3. Positional Encoding:\n",
        "\n",
        "    * Memberikan informasi posisi karena tidak ada struktur sekuens eksplisit dalam arsitektur Transformer"
      ],
      "metadata": {
        "id": "zsY6BVU4Y1me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaled Dot-Product Attention\n",
        "Rumus:\n",
        "\n",
        "\\[\n",
        "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
        "\\]\n",
        "\n",
        "- \\($ Q $\\): Query\n",
        "- \\($ K $\\): Key\n",
        "- \\($ V $\\): Value\n",
        "- \\($ d_k $\\): Dimensi key\n",
        "\n",
        "Penjelasan:\n",
        "* Hitung skor kesamaan antara Query dan Key\n",
        "\n",
        "* Gunakan hasilnya untuk menggabungkan Value secara linier, menghasilkan fokus perhatian yang disesuaikan\n",
        "\n",
        "## Multi-Head Attention\n",
        "Alih-alih menggunakan satu mekanisme perhatian, Transformer memanfaatkan beberapa \"head\" paralel:\n",
        "\n",
        "\\[\n",
        "$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$\n",
        "\\]\n",
        "\n",
        "* Setiap head dapat menangkap aspek representasi yang berbeda dari urutan input\n",
        "\n",
        "## Positional Encoding\n",
        "Karena Transformer tidak memiliki informasi urutan secara implisit seperti RNN, digunakan encoding sinusoidal sebagai berikut:\n",
        "\n",
        "\\[\n",
        "$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$\n",
        "\\]\n",
        "\n",
        "## Struktur Encoder\n",
        "Setiap layer encoder terdiri dari:\n",
        "\n",
        "1. Multi-Head Self-Attention\n",
        "\n",
        "2. Add & Layer Normalization\n",
        "\n",
        "3. Feedforward Neural Network:\n",
        "\n",
        "\\[\n",
        "$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$\n",
        "\\]\n",
        "\n",
        "4. Add & Layer Normalization (ulang)"
      ],
      "metadata": {
        "id": "uYsN37E5ZSnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Struktur Decoder\n",
        "Mirip dengan encoder, tetapi ditambahkan dua komponen:\n",
        "\n",
        "* Masked Self-Attention: untuk mencegah model melihat token berikutnya saat pelatihan\n",
        "\n",
        "* Encoder–Decoder Attention: memungkinkan decoder memerhatikan representasi output dari encoder\n",
        "\n",
        "## Pretraining dan Fine-Tuning\n",
        "### Pretraining\n",
        "Model dilatih terlebih dahulu pada tugas-tugas umum berbasis data skala besar seperti:\n",
        "\n",
        "* Language Modeling (contoh: GPT)\n",
        "\n",
        "* Masked Language Modeling (contoh: BERT)\n",
        "\n",
        "* Translation Modeling (contoh: T5)\n",
        "\n",
        "Tujuannya adalah untuk membangun representasi bahasa umum.\n",
        "\n",
        "### Fine-Tuning\n",
        "Model yang telah dipra-latih kemudian disesuaikan (fine-tuned) untuk tugas-tugas spesifik seperti:\n",
        "\n",
        "* Klasifikasi teks\n",
        "\n",
        "* Named Entity Recognition (NER)\n",
        "\n",
        "* Question Answering\n",
        "\n",
        "Fine-tuning biasanya membutuhkan lebih sedikit data dan lebih efisien."
      ],
      "metadata": {
        "id": "nOPftmo5Zx3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT: Bidirectional Encoder Representations from Transformers\n",
        "* Menggunakan hanya encoder stack\n",
        "\n",
        "* Dilatih dengan dua tugas utama:\n",
        "\n",
        "    * Masked Language Modeling (MLM): beberapa token dalam kalimat diganti dengan [MASK], dan model diminta menebak token aslinya\n",
        "\n",
        "    * Next Sentence Prediction (NSP): memprediksi apakah dua kalimat berurutan secara logis\n",
        "\n",
        "## GPT: Generative Pretrained Transformer\n",
        "* Menggunakan hanya decoder stack\n",
        "\n",
        "* Dilatih dengan causal language modeling: memprediksi token berikutnya secara autoregresif\n",
        "\n",
        "Cocok untuk:\n",
        "\n",
        "* Teks generatif\n",
        "\n",
        "* Kode program\n",
        "\n",
        "* Chatbot atau dialog\n",
        "\n",
        "## T5: Text-To-Text Transfer Transformer\n",
        "* Semua tugas diperlakukan dalam format input dan output berupa teks\n",
        "\n",
        "Contoh:\n",
        "\n",
        "* Input: translate English to German: That is good.\n",
        "\n",
        "* Output: Das ist gut.\n",
        "\n",
        "## Masking dan Causal Attention\n",
        "* Padding Mask: mencegah perhatian terhadap token padding\n",
        "\n",
        "* Look-ahead Mask: mencegah model melihat token masa depan saat training autoregresif"
      ],
      "metadata": {
        "id": "z8myWlkpaBEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning dalam NLP\n",
        "Strategi umum:\n",
        "\n",
        "* Pretraining dengan miliaran token\n",
        "\n",
        "* Fine-tuning dengan ribuan token\n",
        "\n",
        "Keuntungan:\n",
        "\n",
        "* Akurasi tinggi\n",
        "\n",
        "* Biaya komputasi lebih rendah\n",
        "\n",
        "* Mengurangi kebutuhan anotasi manual secara signifikan\n",
        "\n",
        "## Evaluasi dan Efisiensi Model\n",
        "Transformers telah meningkatkan performa pada banyak benchmark NLP, seperti SQuAD dan GLUE.\n",
        "\n",
        "### Teknik Optimisasi:\n",
        "* Mixed precision training\n",
        "\n",
        "* Knowledge distillation\n",
        "\n",
        "* Parameter sharing (contoh: ALBERT)\n",
        "\n",
        "# **Kesimpulan**\n",
        "* Transformer telah menggantikan RNN untuk mayoritas tugas NLP\n",
        "\n",
        "* Mekanisme self-attention memungkinkan pemrosesan paralel dan efisien\n",
        "\n",
        "* Model pretrained seperti BERT, GPT, dan T5 mendominasi aplikasi NLP modern\n",
        "\n",
        "* Fine-tuning memungkinkan adaptasi cepat dan akurat, bahkan dengan data terbatas\n",
        "\n",
        "# **Referensi**\n",
        "Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O'Reilly Media."
      ],
      "metadata": {
        "id": "8wMTa6PJaSav"
      }
    }
  ]
}