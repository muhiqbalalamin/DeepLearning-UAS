{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO13TJRcrBdeAR0S4YIhq0N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhiqbalalamin/DeepLearning-UAS/blob/main/Chapter_16_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Language Processing with RNNs and Attention**"
      ],
      "metadata": {
        "id": "Fsdtsv_PWe02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tujuan Bab\n",
        "Bab ini membahas bagaimana membangun sistem pemrosesan bahasa alami (Natural Language Processing / NLP) menggunakan:\n",
        "\n",
        "* Recurrent Neural Networks (RNN)\n",
        "\n",
        "* LSTM dan GRU\n",
        "\n",
        "* Arsitektur Encoder–Decoder\n",
        "\n",
        "* Mekanisme Attention\n",
        "\n",
        "## NLP dan Representasi Teks\n",
        "Teks tidak dapat langsung digunakan sebagai input model dan perlu diubah menjadi representasi numerik.\n",
        "\n",
        "### Tokenisasi\n",
        "* Memecah teks menjadi unit-unit kecil seperti kata, sub-kata, atau karakter.\n",
        "\n",
        "### Vektorisasi\n",
        "* One-hot encoding: Representasi vektor sparse dan berdimensi besar.\n",
        "\n",
        "* Word embeddings: Representasi vektor densitas tetap, seperti:\n",
        "\n",
        "    * Word2Vec\n",
        "\n",
        "    * GloVe\n",
        "\n",
        "    * FastText\n",
        "\n",
        "### Embedding Layer\n",
        "\\[\n",
        "$E: \\text{Vocab} \\rightarrow \\mathbb{R}^d$\n",
        "\\]\n",
        "\n",
        "Layer ini mempelajari representasi vektor berdimensi tetap dari setiap token selama proses pelatihan."
      ],
      "metadata": {
        "id": "Yi9Ev-jCWhEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arsitektur Encoder–Decoder\n",
        "Biasa digunakan untuk tugas-tugas sequence-to-sequence, seperti:\n",
        "\n",
        "* Penerjemahan mesin (machine translation)\n",
        "\n",
        "* Peringkasan teks (summarization)\n",
        "\n",
        "* Generasi teks\n",
        "\n",
        "### Encoder:\n",
        "Mengubah urutan input menjadi sebuah vektor konteks tetap:\n",
        "\n",
        "\\[\n",
        "$\\mathbf{c} = h_T$\n",
        "\\]\n",
        "\n",
        "### Decoder:\n",
        "Menghasilkan output sequence dari konteks:\n",
        "\n",
        "\\[\n",
        "$s_t = f(s_{t-1}, y_{t-1}, \\mathbf{c})$\n",
        "\\]\n",
        "\n",
        "Masalah:\n",
        "Vektor konteks berdimensi tetap sulit untuk menangkap seluruh informasi, terutama pada urutan panjang.\n",
        "Solusi: Attention mechanism."
      ],
      "metadata": {
        "id": "O_UA61w2XI42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mekanisme Attention\n",
        "Membantu model memfokuskan perhatian pada bagian penting dari input saat menghasilkan setiap token output.\n",
        "\n",
        "### Langkah-langkah Umum:\n",
        "1. Untuk setiap timestep decoder \\($ t $\\), hitung perhatian terhadap encoder hidden states \\($ h_1, ..., h_T $\\)\n",
        "2. Hitung skor perhatian:\n",
        "\n",
        "\\[\n",
        "$score(s_t, h_i) = s_t^T W_a h_i$\n",
        "\\]\n",
        "\n",
        "3. Hitung bobot perhatian dengan softmax:\n",
        "\n",
        "\\[\n",
        "$\\alpha_{t,i} = \\frac{\\exp(score(s_t, h_i))}{\\sum_j \\exp(score(s_t, h_j))}$\n",
        "\\]\n",
        "\n",
        "4. Hitung vektor konteks dinamis:\n",
        "\n",
        "\\[\n",
        "$c_t = \\sum_i \\alpha_{t,i} h_i$\n",
        "\\]\n",
        "\n",
        "5. Decoder menghasilkan output berdasarkan \\($ s_{t-1}, y_{t-1}, c_t $\\)"
      ],
      "metadata": {
        "id": "3YmYhe-LXZSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jenis Mekanisme Attention\n",
        "### Bahdanau Attention (Additive)\n",
        "\\[\n",
        "$score(s_t, h_i) = v_a^T \\tanh(W_s s_t + W_h h_i)$\n",
        "\\]\n",
        "\n",
        "###  Luong Attention (Multiplicative / Dot Product)\n",
        "\n",
        "\\[\n",
        "$score(s_t, h_i) = s_t^T W h_i$\n",
        "\\]\n",
        "\n",
        "## Aplikasi Attention dalam NLP\n",
        "1. Machine Translation\n",
        "\n",
        "    * Input: kalimat dalam bahasa sumber\n",
        "\n",
        "    * Output: kalimat dalam bahasa target\n",
        "\n",
        "    * Attention sangat penting untuk menangani input panjang\n",
        "\n",
        "2. Text Summarization\n",
        "\n",
        "    * Input: dokumen\n",
        "\n",
        "    * Output: ringkasan singkat\n",
        "\n",
        "3. Text Generation\n",
        "\n",
        "    * Model: kombinasi RNN + embedding + attention\n",
        "\n",
        "4. Question Answering\n",
        "\n",
        "    * Model difokuskan pada bagian teks yang relevan dengan pertanyaan"
      ],
      "metadata": {
        "id": "BQiaCrO5Xn9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding Pretrained\n",
        "Gunakan representasi kata dari model yang telah dilatih sebelumnya, seperti:\n",
        "\n",
        "* GloVe\n",
        "\n",
        "* FastText\n",
        "\n",
        "* BERT (contextual embedding)\n",
        "\n",
        "Keunggulan:\n",
        "\n",
        "* Mengurangi kebutuhan data besar\n",
        "\n",
        "* Mempercepat proses pelatihan (konvergensi)\n",
        "\n",
        "## Contextual Embeddings\n",
        "Model modern menghasilkan embedding yang mempertimbangkan konteks, contohnya:\n",
        "\n",
        "* ELMo\n",
        "\n",
        "* BERT\n",
        "\n",
        "* GPT\n",
        "\n",
        "Contoh: Kata “bank” dalam “river bank” ≠ “financial bank”.\n",
        "\n",
        "## Teacher Forcing\n",
        "Selama pelatihan decoder, digunakan token target sebagai input alih-alih output prediksi sebelumnya.\n",
        "\n",
        "* Mempercepat pelatihan\n",
        "\n",
        "* Namun dapat menyebabkan exposure bias saat inferensi (model terlalu bergantung pada ground truth)\n",
        "\n",
        "## Beam Search (Inferensi)\n",
        "Selama prediksi:\n",
        "\n",
        "* Alih-alih memilih satu token terbaik secara greedy, simpan k kandidat terbaik (beam size).\n",
        "\n",
        "* Membantu menyeimbangkan akurasi dan efisiensi eksplorasi hasil prediksi.\n",
        "\n",
        "## Evaluasi dalam NLP\n",
        "1. BLEU Score – untuk penerjemahan\n",
        "\n",
        "2. ROUGE – untuk peringkasan teks\n",
        "\n",
        "3. Perplexity – untuk pemodelan bahasa\n",
        "\n",
        "4. Akurasi – untuk klasifikasi teks"
      ],
      "metadata": {
        "id": "Z3LqVNcjYAGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kesimpulan**\n",
        "* Representasi numerik teks (embedding) merupakan dasar utama dalam NLP.\n",
        "\n",
        "* Arsitektur encoder–decoder efektif untuk tugas sequence-to-sequence.\n",
        "\n",
        "* Mekanisme attention memungkinkan pemfokusan konteks secara dinamis.\n",
        "\n",
        "* Embedding pra-latih dan model kontekstual modern meningkatkan performa NLP.\n",
        "\n",
        "* Teknik seperti teacher forcing dan beam search mengoptimalkan proses pelatihan dan inferensi.\n",
        "\n",
        "# **Referensi**\n",
        "Géron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O'Reilly Media."
      ],
      "metadata": {
        "id": "3Ox6_0WRYaKt"
      }
    }
  ]
}