{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtvfjg8NhXQOraNFPjjP+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhiqbalalamin/DeepLearning-UAS/blob/main/Chapter_18_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reinforcement Learning (RL)**"
      ],
      "metadata": {
        "id": "9MPf-jcyaxFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tujuan Bab\n",
        "Bab ini membahas dasar-dasar Reinforcement Learning (RL), yaitu pendekatan Machine Learning di mana agen belajar melalui interaksi dengan lingkungan guna memaksimalkan reward jangka panjang.\n",
        "\n",
        "* RL banyak diterapkan dalam:\n",
        "\n",
        "* Permainan (misalnya: AlphaGo, Dota 2 AI)\n",
        "\n",
        "* Robotika\n",
        "\n",
        "* Sistem rekomendasi adaptif\n",
        "\n",
        "* Navigasi otonom\n",
        "\n",
        "## Perbedaan RL dan Supervised Learning\n",
        "\n",
        "| **Supervised Learning**          | **Reinforcement Learning**                  |\n",
        "| -------------------------------- | ------------------------------------------- |\n",
        "| Diberikan label secara eksplisit | Tidak ada label eksplisit, hanya reward     |\n",
        "| Fokus pada prediksi satu langkah | Melibatkan interaksi multi-langkah          |\n",
        "| Tidak interaktif                 | Interaktif: tindakan memengaruhi lingkungan |\n",
        "\n",
        "## Komponen Utama RL\n",
        "1. Agent: entitas yang belajar dan mengambil keputusan\n",
        "\n",
        "2. Environment: lingkungan tempat agent berinteraksi\n",
        "\n",
        "3. State (s): representasi kondisi lingkungan saat ini\n",
        "\n",
        "4. Action (a): tindakan yang dapat diambil oleh agent\n",
        "\n",
        "5. Reward (r): umpan balik dari lingkungan atas aksi agent\n",
        "\n",
        "6. Policy (Ï€): strategi yang mengatur pemilihan aksi berdasarkan state\n",
        "\n",
        "7. Value Function (V(s)): ekspektasi total reward dari suatu state\n",
        "\n",
        "8. Q-Function (Q(s, a)): ekspektasi total reward dari pasangan state dan action\n",
        "\n"
      ],
      "metadata": {
        "id": "tS4JivAma2He"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proses RL â€“ Markov Decision Process (MDP)\n",
        "Lingkungan dalam RL umumnya dimodelkan sebagai Markov Decision Process:\n",
        "\n",
        "\\[\n",
        "$(S, A, P, R, \\gamma)$\n",
        "\\]\n",
        "\n",
        "* \\($ S $\\): ruang state\n",
        "* \\($ A $\\): ruang aksi\n",
        "* \\($ P(s'|s,a) $\\): probabilitas transisi dari state ğ‘  ke 'ğ‘  â€² dengan aksi ğ‘\n",
        "* \\($ R(s,a) $\\): reward yang diterima atas aksi ğ‘ di state ğ‘ \n",
        "* \\($ \\gamma $\\): discount factor, mengatur pentingnya reward masa depan (0 < Î³ â‰¤ 1)\n",
        "\n",
        "## Tujuan RL\n",
        "Maksimalisasi cumulative expected reward:\n",
        "\n",
        "\\[\n",
        "$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$\n",
        "\\]\n",
        "\n",
        "Agent belajar policy ğœ‹(ğ‘âˆ£ğ‘ ) yang memaksimalkan nilai tersebut.\n",
        "\n",
        "\n",
        "##  Fungsi Nilai (Value Function)\n",
        "\n",
        "### State Value Function:\n",
        "\n",
        "\\[\n",
        "$V^\\pi(s) = \\mathbb{E}_\\pi [G_t | S_t = s]$\n",
        "\\]\n",
        "\n",
        "###  Action Value Function (Q-Value):\n",
        "\n",
        "\\[\n",
        "$Q^\\pi(s, a) = \\mathbb{E}_\\pi [G_t | S_t = s, A_t = a]$\n",
        "\\]\n",
        "\n",
        "## Persamaan Bellman\n",
        "\n",
        "### Untuk \\($ V^\\pi(s) $\\):\n",
        "\n",
        "\\[\n",
        "$V^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^\\pi(s')]$\n",
        "\\]\n",
        "\n",
        "### Untuk \\($ Q^\\pi(s,a) $\\):\n",
        "\n",
        "\\[\n",
        "$Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s',a')]$\n",
        "\\]\n"
      ],
      "metadata": {
        "id": "jFzXmnymbEbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pendekatan Policy-Based vs Value-Based\n",
        "* Policy-based: langsung mempelajari policy Ï€ (contoh: REINFORCE, PPO)\n",
        "\n",
        "* Value-based: mempelajari fungsi nilai terlebih dahulu, lalu menurunkan policy (contoh: Q-Learning, DQN)\n",
        "\n",
        "## Q-Learning\n",
        "Algoritma off-policy yang memperbarui nilai Q sebagai berikut:\n",
        "\n",
        "\\[\n",
        "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s,a)]$\n",
        "\\]\n",
        "\n",
        "* \\($ \\alpha $\\): learning rate\n",
        "\n",
        "---\n",
        "\n",
        "##  Deep Q-Network (DQN)\n",
        "Menggunakan Neural Network untuk mengaproksimasi fungsi Q:\n",
        "\n",
        "\\[\n",
        "$Q(s, a; \\theta) \\approx Q^\\ast(s, a)$\n",
        "\\]\n",
        "\n",
        "Fitur penting:\n",
        "\n",
        "* Experience Replay: menyimpan pengalaman dalam buffer untuk pelatihan batch\n",
        "\n",
        "* Target Network: menjaga stabilitas dengan parameter target yang tidak berubah cepat\n",
        "\n",
        "##  Policy Gradient\n",
        "\n",
        "Secara langsung mengoptimalkan parameter policy \\($ \\theta $\\):\n",
        "\n",
        "\\[\n",
        "$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^\\pi(s,a)]$\n",
        "\\]\n",
        "\n",
        "Contoh: algoritma REINFORCE\n",
        "\n",
        "\n",
        "##  Actorâ€“Critic\n",
        "\n",
        "Gabungkan:\n",
        "* Actor: memperbarui policy \\($ \\pi $\\)\n",
        "* Critic: mengevaluasi policy melalui fungsi nilai\n",
        "\n",
        "Lebih stabil dibandingkan metode policy gradient murni."
      ],
      "metadata": {
        "id": "U4GQvJ_RcCEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Advantage Function\n",
        "\n",
        "Mengukur keunggulan suatu aksi dibandingkan ekspektasi rata-rata dari state:\n",
        "\n",
        "\\[\n",
        "$A(s,a) = Q(s,a) - V(s)$\n",
        "\\]\n",
        "\n",
        "Digunakan dalam algoritma seperti A2C dan PPO\n",
        "\n",
        "## Algoritma Modern dalam RL\n",
        "* DQN: berbasis nilai, menggunakan Deep Learning\n",
        "\n",
        "* Double DQN: mengurangi estimasi berlebih pada Q-value\n",
        "\n",
        "* Dueling DQN: memisahkan perhitungan nilai state dan keunggulan aksi\n",
        "\n",
        "* PPO (Proximal Policy Optimization): policy gradient yang stabil dan efisien\n",
        "\n",
        "* A2C / A3C: Actorâ€“Critic paralel untuk efisiensi pelatihan\n",
        "\n",
        "# **Kesimpulan**\n",
        "* RL adalah paradigma pembelajaran berbasis interaksi agen dengan lingkungan dan feedback berupa reward\n",
        "\n",
        "* Tujuan utama adalah menemukan policy optimal yang memaksimalkan reward kumulatif\n",
        "\n",
        "* Pendekatan yang tersedia mencakup Q-learning, policy gradient, dan actorâ€“critic\n",
        "\n",
        "* Deep Reinforcement Learning memungkinkan penyelesaian masalah kompleks seperti game dan kontrol robotik\n",
        "\n",
        "# **Referensi**\n",
        "GÃ©ron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O'Reilly Media.\n",
        "\n"
      ],
      "metadata": {
        "id": "qeQVxrDlcvKn"
      }
    }
  ]
}
